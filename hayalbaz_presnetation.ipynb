{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Discover {hayalbaz}\"\n",
        "author: \"Vestin Hategekimana\"\n",
        "date: \"02-05-2023\"\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: dark\n",
        "editor: visual\n",
        "---"
      ],
      "id": "fd6aa5c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{css, echo = FALSE}\n",
        ".justify {\n",
        "  text-align: justify !important\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "# A few words about me\n",
        "1. PhD student (Demography) - IDESO (UNIGE)\n",
        "2. Member of [WeData](https://wedata.ch/)\n",
        "3. Fan of web scraping, text mining, network analysis and data visualization\n",
        "4. My programming languages: \n",
        "    1. R, Nim and Julia \n",
        "    2. Python and JavaScript\n",
        "\n",
        "# Plan\n",
        "1. What is web scraping?\n",
        "2. Web scraping in R\n",
        "3. What is {hayalbaz}?\n",
        "4. Advantages of {hayalbaz}\n",
        "\n",
        "# What is web scraping?\n",
        "## Definition\n",
        "\n",
        "::: {.justify}\n",
        "\n",
        "**Web scraping** is the process of extracting information from websites and converting it into a structured format, such as a spreadsheet or a database. This is usually done programmatically using a software tool or script, which simulates human browsing behavior to navigate through web pages, identify relevant data, and extract the desired information.\n",
        "\n",
        ":::\n",
        "\n",
        "**Main goal: collect data** \n",
        "\n",
        "## Web crawling?\n",
        "Web crawling is the process of systematically browsing, discovering, and indexing the content of websites. This is typically done by automated programs called web crawlers, spiders, or bots. Web crawling is a crucial component of search engines, as it allows them to build a comprehensive index of web pages that can be used to serve relevant search results to users.\n",
        "\n",
        "**Main goal: index and/or discover content by following urls**\n",
        "\n",
        "## What is it used for?\n",
        "- Data mining/analysis\n",
        "- Content aggregation\n",
        "- Sentiment analysis\n",
        "- Price comparaison\n",
        "\n",
        "## Where is it used?\n",
        "- Business (analytics)\n",
        "\t- Competitor\n",
        "\t- Profiling\n",
        "\t- Finance\n",
        "- Research\n",
        "\t- Analysis purpose\n",
        "- Private\n",
        "\t- Automation\n",
        "\n",
        "## Example\n",
        "(==Reuse my scraper of le temps.ch==)\n",
        "\n",
        "## Is it hard?\n",
        "It depends...\n",
        "\n",
        "**We can use programming language or tools to make it easy**\n",
        "\n",
        "**It is not always required but is always good to have a bit of knowledge in HTML, CSS and JavaScript**\n",
        "\n",
        "**With modern tools, we can virtually scrape almost any website!!!**\n",
        "\n",
        "## Limits\n",
        "- **Legal and ethical considerations**: Web scraping may infringe on copyright, trademark, or other intellectual property rights. Additionally, it might violate a website's terms of service, privacy policies, or other legal agreements. It's essential to understand and comply with the legal and ethical aspects of web scraping to avoid potential legal issues.\n",
        "\t- Website term of use\n",
        "\t- Privacy policies\n",
        "\t- Intellectual property, copyright or trademark\n",
        "\n",
        "- **Data quality and accuracy**: Web scraping can sometimes result in inaccurate or incomplete data, especially when dealing with poorly structured or inconsistent websites. Data cleaning and validation are often necessary to ensure the quality of the extracted data.\n",
        "\n",
        "- **Website structure changes**: Websites frequently change their structure, layout, or design. This can cause web scrapers to break or become unreliable, requiring constant maintenance and updates to keep up with website changes.\n",
        "\n",
        "- **Dynamic and JavaScript-heavy websites**: Web scraping can be challenging when dealing with websites that rely heavily on JavaScript for content loading or have dynamic elements. Traditional web scraping libraries might not be able to extract data from such websites, and alternative approaches, such as using browser automation tools like Selenium or Playwright, may be needed.\n",
        "\t- Javascript content loading\n",
        "\t- Dynamic/interactive element\n",
        "\n",
        "- **Rate limiting and server load**: Sending too many requests in a short period can overwhelm a website's server or trigger rate-limiting mechanisms. It's essential to respect a website's server resources and limit the rate of requests to avoid potential issues or bans.\n",
        "\t- Slow down / Block\n",
        "\t- DDOS attack\n",
        "\n",
        "- **Anti-scraping techniques**: Many websites employ anti-scraping techniques to prevent or limit web scraping, such as CAPTCHAs, IP blocking, user-agent restrictions, or requiring login credentials. Overcoming these obstacles can be challenging and may require more advanced scraping techniques or proxy services.\n",
        "\t- CAPTCHA\n",
        "\t- IP blocking\n",
        "\t- Login\n",
        "\t- User-agent restriction\n",
        "\n",
        "## Good practice\n",
        "1. **Limit the rate of your requests** to avoid putting strain on the website's server.\n",
        "2. **Identify your scraper in the User-Agent header** when sending requests.\n",
        "3. **Respect the guidelines** provided in the robots.txt file.\n",
        "4. **Cache data whenever possible** to minimize the number of requests you need to send.\n",
        "\n",
        "# Ethical consideration\n",
        "## Is it legal?\n",
        "**It depends... **\n",
        "- A grey area? Not really...\n",
        "\n",
        "**It is legal for publicly available data on the internet.**\n",
        "\n",
        "**But be careful with:**\n",
        "- Personal data\n",
        "- Data under intellectual property\n",
        "\n",
        "**Risks:**\n",
        "- From simple warning to legal actions\n",
        "\n",
        "## How to check?\n",
        "Robot.txt\n",
        "- ex: https://www.letemps.ch/robots.txt\n",
        "\n",
        "Website term of use\n",
        "- ex: https://twitter.com/en/tos\n",
        "\n",
        "# Tools for web scraping\n",
        "## Code, no code?\n",
        "**Nowadays there are tools in web browser to perform web scraping tasks**\n",
        "ex. [web scraper](https://chrome.google.com/webstore/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn)\n",
        "\n",
        "**Using a programming language gives you more flexibility**\n",
        "\n",
        "## Best programming language?\n",
        "Classical ranking on internet:\n",
        "1. Python\n",
        "2. JavaScript\n",
        "3. Ruby\n",
        "4. Java\n",
        "5. PhP\n",
        "\n",
        "**No places for R... But it doesn't mean there is nothing!!**\n",
        "\n",
        "## Web scraping in R\n",
        "Packages for web scraping:\n",
        "(exclude web server, json tools, etc.)\n",
        "- [htmldf](https://www.ebay.com/): Collect pages Metadata\n",
        "- [scrappy](https://github.com/villegar/scrappy/): Scraping helper for specific web sites\n",
        "- [ragler](https://github.com/feddelegrand7/ralger): Easiest rvest\n",
        "- [parsel](https://github.com/till-tietz/parsel): Parallel processing for RSelenium\n",
        "- [rvest](https://rvest.tidyverse.org/): Reference in R (wrap httr and XML packages)\n",
        "- [scraEP](https://github.com/cran/scraEP): Little tools for web scraping\n",
        "- [Rcrawler](https://github.com/salimk/Rcrawler/): A crawler for R\n",
        "- [curl](https://github.com/jeroen/curl): Modern web curl\n",
        "- [chromote](https://github.com/rstudio/chromote): Headless chrome driver\n",
        "\t- [crrri](https://github.com/RLesur/crrri)\n",
        "\t- [decapited](https://github.com/hrbrmstr/decapitated/)\n",
        "\t- [chradle](https://github.com/milesmcbain/chradle)\n",
        "\n",
        "## Most popular Packages\n",
        "For API (Google Map API)\n",
        "- [httr](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html) / [httr2](https://cran.r-project.org/web/packages/httr2/vignettes/httr2.html)\n",
        "For static web site (Wikipedia)\n",
        "- [rvest](https://rvest.tidyverse.org/)\n",
        "For dynamic web site (Twitter)\n",
        "- [Rselenium](https://docs.ropensci.org/RSelenium/)\n",
        "\n",
        "## Static vs Dynamic method\n",
        "**Static method (rvest) simply do request and collect basic html pages**\n",
        "**Pros**\n",
        "- Fast\n",
        "- Light\n",
        "- Most web site are static\n",
        "**Cons**\n",
        "- Can't load JavaScript\n",
        "- Easily blocked by antibot methods\n",
        "\n",
        "**Dynamic method (RSelenium) automate a web browser to render website (JavaScript)**\n",
        "**Pros**\n",
        "- Can render JavaScript\n",
        "- Can avoid anti-bot methods\n",
        "- Can deal with interactive website (trigger events)\n",
        "**Cons**\n",
        "- Slow\n",
        "- Heavy\n",
        "\n",
        "## Example with Rvest\n",
        "### Installation\n",
        "\n",
        "### Collection\n",
        "\n",
        "## Example with RSelenium\n",
        "### Installation\n",
        "\n",
        "### Collection\n",
        "\n",
        "## Limits of RSelenium\n",
        "- Dependencies (Need Java)\n",
        "\t- Not easily shareable\n",
        "\t- Can be discouraging\n",
        "- Not beginner friendly\n",
        "\t- Verbose\n",
        "\n",
        "# What is {hayalbaz}\n",
        "## {hayalbaz}\n",
        "Author: [Colind Rundel](https://github.com/rundel) \n",
        "\"Puppeteer in a different language - this R package provides a puppeteer inspired interface to the Chrome Devtools Protocol using chromote.\"\n",
        "\n",
        "- What is Puppeteer?\n",
        "- What is chromote?\n",
        "\n",
        "\n",
        "## Puppeteer\n",
        "[Puppeteer](https://pptr.dev/)\n",
        "\"Puppeteer is a Node.js library which provides a high-level API to control Chrome/Chromium over the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). Puppeteer runs in [headless](https://developer.chrome.com/articles/new-headless/) mode by default, but can be configured to run in full (non-headless) Chrome/Chromium.\"\n",
        "\n",
        "## {chromote}\n",
        "[chromote](https://github.com/rstudio/chromote)\n",
        "Chromote is an R implementation of the [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). It works with Chrome, Chromium, Opera, Vivaldi, and other browsers based on [Chromium](https://www.chromium.org/). By default it uses Google Chrome (which must already be installed on the system). To use a different browser, see [Specifying which browser to use](https://github.com/rstudio/chromote#specifying-which-browser-to-use).\n",
        "\n",
        "Chromote is not the only R package that implements the Chrome DevTools Protocol. Here are some others:\n",
        "- [crrri](https://github.com/RLesur/crrri) by Romain Lesur and Christophe Dervieux\n",
        "- [decapitated](https://github.com/hrbrmstr/decapitated/) by Bob Rudis\n",
        "- [chradle](https://github.com/milesmcbain/chradle) by Miles McBain\n",
        "\n",
        "The interface to Chromote is similar to [chrome-remote-interface](https://github.com/cyrus-and/chrome-remote-interface) for node.js.\n",
        "\n",
        "## Web scraping with chromote?\n",
        "Yes, but really verbose\n",
        "Example:\n",
        "==give a code example==\n",
        "\n",
        "## hayalbaz make it easier\n",
        "==Same example as before with hayalbaz==\n",
        "\n",
        "## Advantage of hayalbaz over RSelenium\n",
        "1. Easy installation\n",
        "2. Less verbose\n",
        "3. Easier overall\n",
        "4. Faster\n",
        "5. Good synergy with Rvest\n",
        "6. Promising features\n",
        "\n",
        "**Note: I don't think the goal of hayalbaz is to replace RSelenium, but it can be a valuable alternative**\n",
        "\n",
        "# Advantages of hayalbaz over RSelenium\n",
        "## 1. Easy installation\n",
        "RSelenium:\n",
        "[The Complete RSelenium Installation Guide (2023)](https://www.youtube.com/watch?v=GnpJujF9dBw)\n",
        "- Need JavaScript installed\n",
        "\n",
        "## 2. Less verbose\n",
        "==Same example as earlier with RSelenium and hayalbaz next to each other==\n",
        "\n",
        "## 3. Faster\n",
        "==Use the script benchmark_rselenium_hayalbaz.R==\n",
        "\n",
        "## 4. Easier overall\n",
        "Because:\n",
        "1. Less installation\n",
        "2. Less verbose\n",
        "3. No port management\n",
        "4. One web browser (you can use chromium too though)\n",
        "5. Self-explanatory methods\n",
        "\n",
        "## 5. Good synergy with rvest\n",
        "==Find an easy example==\n",
        "\n",
        "## 6. Promising feature\n",
        "**Do you know the {shinytest2}?**\n",
        "To test applications automatically.\n",
        "\n",
        "[shinytest2](https://rstudio.github.io/shinytest2/) uses [chromote](https://github.com/rstudio/chromote) to render applications in a headless Chrome browser. [chromote](https://github.com/rstudio/chromote) allows for a live preview, better debugging tools, and/or simply using modern JavaScript/CSS.\n",
        "\n",
        "By simply recording your actions as code and extending them to test the more particular aspects of your application, it will result in fewer bugs and more confidence in future Shiny application development.\n",
        "\n",
        "### Example\n",
        "Create\n",
        "\n",
        "```r\n",
        "shinytest2::record_test()\n",
        "```\n",
        "\n",
        "### Like puppeteer or playwright\n",
        "Recording action to:\n",
        "1. Save time\n",
        "2. Be more beginner friendly\n",
        "\n",
        "**It is still in the project stage**\n",
        "\n",
        "**Again, the goal is not to replace RSelenium but it can be a valuable alternative**\n",
        "\n",
        "# Thank you for your attention"
      ],
      "id": "e7a86242"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}