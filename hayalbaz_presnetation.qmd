---
title: "Discover {hayalbaz}"
author: "Vestin Hategekimana"
date: "`r Sys.Date()`"
execute: 
  echo: true
  warning: false
format: 
  revealjs:
    theme: dark
editor: visual
---

# A few words about me

1.  PhD student (Demography) - IDESO (UNIGE)
2.  Member of [WeData](https://wedata.ch/)
3.  Fan of web scraping, text mining, network analysis and data visualization
4.  My programming languages:
    1.  R, Nim and Julia
    2.  Python and JavaScript

# Plan

1.  What is web scraping?
2.  Web scraping in R
3.  What is {hayalbaz}?
4.  Advantages of {hayalbaz}

# What is web scraping?

## Web scraping?

**Web scraping** is the process of extracting information from websites and converting it into a structured format, such as a spreadsheet or a database.

1.  Navigate through web pages
2.  Identify relevant data
3.  Extract the desired information

**Main goal: collect data**

## Web crawling?

Web crawling is the process of systematically browsing, discovering, and indexing the content of websites. This is typically done by automated programs called web crawlers, spiders, or bots.

-   Used by search engine

**Main goal: index and/or discover content by following urls**

## What is web scraping used for?

-   Data mining/analysis
-   Content aggregation

## Where is it used?

-   Business (analytics)
    -   Competitor
    -   Profiling
    -   Finance
-   Research
    -   Analysis purpose
-   Private
    -   Automation

## Example

I created a function that allow me to research news articles in the website "letemps.ch" based on a specific keyword.

Here is the example...

```{r echo=FALSE}
# TODO Find/Create the code for letemps.ch
```

## Is web scraping hard?

It depends...

-   **We can use programming language or tools to make it easy**

-   **It is not always required but is always good to have a bit of knowledge in HTML, CSS and JavaScript**

-   **With modern tools, we can virtually scrape almost any website!!!**

# Limits to web scraping

## Limits (1)

-   **Legal and ethical considerations**: Web scraping may infringe on copyright, trademark, or other intellectual property rights. Additionally, it might violate a website's terms of service, privacy policies, or other legal agreements. It's essential to understand and comply with the legal and ethical aspects of web scraping to avoid potential legal issues.
    -   Website term of use
    -   Privacy policies
    -   Intellectual property, copyright or trademark

## Limits (2)

-   **Data quality and accuracy**: Web scraping can sometimes result in inaccurate or incomplete data, especially when dealing with poorly structured or inconsistent websites. Data cleaning and validation are often necessary to ensure the quality of the extracted data.

-   **Website structure changes**: Websites frequently change their structure, layout, or design. This can cause web scrapers to break or become unreliable, requiring constant maintenance and updates to keep up with website changes.

## Limits (3)

-   **Dynamic and JavaScript-heavy websites**: Web scraping can be challenging when dealing with websites that rely heavily on JavaScript for content loading or have dynamic elements. Traditional web scraping libraries might not be able to extract data from such websites, and alternative approaches, such as using browser automation tools like Selenium or Playwright, may be needed.
    -   Javascript content loading
    -   Dynamic/interactive element

## Limits (4)

-   **Rate limiting and server load**: Sending too many requests in a short period can overwhelm a website's server or trigger rate-limiting mechanisms. It's essential to respect a website's server resources and limit the rate of requests to avoid potential issues or bans.
    -   Slow down / Block
    -   DDOS attack

## Limits (5)

-   **Anti-scraping techniques**: Many websites employ anti-scraping techniques to prevent or limit web scraping, such as CAPTCHAs, IP blocking, user-agent restrictions, or requiring login credentials. Overcoming these obstacles can be challenging and may require more advanced scraping techniques or proxy services.
    -   CAPTCHA
    -   IP blocking
    -   Login
    -   User-agent restriction

## Good practices

1.  **Limit the rate of your requests** to avoid putting strain on the website's server.
2.  **Identify your scraper in the User-Agent header** when sending requests.
3.  **Respect the guidelines** provided in the robots.txt file.
4.  **Cache data whenever possible** to minimize the number of requests you need to send.

# Ethical consideration

## Is web scraping legal?

**It depends...**

-   A grey area? Not really...

**It is legal for publicly available data on the internet.**

**But be careful with:**

-   Personal data
-   Data under intellectual property

**Risks:**

-   From simple warning to legal actions

## How to check if it is ok to scrape a website?

Robot.txt

-   ex: <https://www.letemps.ch/robots.txt>

Website term of use

-   ex: <https://twitter.com/en/tos>

# Tools for web scraping

## Code, no code?

**Nowadays there are tools in web browser to perform web scraping tasks** ex. [web scraper](https://chrome.google.com/webstore/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn)

**Using a programming language gives you more flexibility**

## Best programming language for web scraping?

Classical ranking on internet:

1.  Python
2.  JavaScript
3.  Ruby
4.  Java
5.  PhP

**No places for R... But it doesn't mean there is nothing!!**

## Web scraping in R (1)

Packages for web scraping: (exclude web server, json tools, etc.)

-   [htmldf](https://www.ebay.com/): Collect pages Metadata
-   [scrappy](https://github.com/villegar/scrappy/): Scraping helper for specific web sites
-   [ragler](https://github.com/feddelegrand7/ralger): Easiest rvest
-   [parsel](https://github.com/till-tietz/parsel): Parallel processing for RSelenium
-   [rvest](https://rvest.tidyverse.org/): Reference in R (wrap httr and XML packages)
-   [scraEP](https://github.com/cran/scraEP): Little tools for web scraping
-   [Rcrawler](https://github.com/salimk/Rcrawler/): A crawler for R

## Web scraping in R (2)

-   [curl](https://github.com/jeroen/curl): Modern web curl
-   [chromote](https://github.com/rstudio/chromote): Headless chrome driver
    -   [crrri](https://github.com/RLesur/crrri)
    -   [decapited](https://github.com/hrbrmstr/decapitated/)
    -   [chradle](https://github.com/milesmcbain/chradle)

## Most popular Packages in R

-   [httr](https://httr.r-lib.org/) / [httr2](https://httr2.r-lib.org/) for API (Google Map API)

-   [rvest](https://rvest.tidyverse.org/) for static web site ([Wikipedia](https://en.wikipedia.org/wiki/Web_scraping))

-   [RSelenium](https://docs.ropensci.org/RSelenium/) for dynamic web site ([Twitter](https://twitter.com/?lang=en))

## Static method

**Static method (rvest) simply do request and collect basic html pages**

**Pros**

-   Fast
-   Light
-   Most web site are static

**Cons**

-   Can't load JavaScript
-   Easily blocked by antibot methods

## Dynamic method

**Dynamic method (RSelenium) automate a web browser to render website (JavaScript)**

**Pros**

-   Can render JavaScript
-   Can avoid anti-bot methods
-   Can deal with interactive website (trigger events)

**Cons**

-   Slow
-   Heavy

## Examples

quote.toscrape is open to web scraping.

Static website:

<http://quotes.toscrape.com/>

Dynamic website:

<http://quotes.toscrape.com/js/>

```{r}
# Static page
url_stat <- "http://quotes.toscrape.com/"

# JavaScript generated content
url_dyn <- "http://quotes.toscrape.com/js/"
```

## Example with rvest

### Installation

```{r eval=FALSE}
# Install
install.packages("rvest")

# Library
library(rvest)
```

```{r echo=FALSE, eval=TRUE}
# Library
library(rvest)
```

------------------------------------------------------------------------

### Collection

```{r}
# Collect html page
page <- read_html(url_stat)

# Print all quotes
page %>% 
  html_elements("span.text") %>% 
  html_text2() %>% 
  head()
```

------------------------------------------------------------------------

### Doesn't works with JavaScript generated content

```{r eval=FALSE}
# Collect html page
page <- read_html(url_dyn) # page with JavaScript

# Print all quotes
page %>% 
  html_elements("span.text") %>% 
  html_text2() %>% 
  head()
```

It shows nothing!

## Example with RSelenium

### Installation

1.  You should have [Java](https://www.azul.com/downloads/) installed in your computer

2.  Then install RSelenium

```{r eval=FALSE}
# Install
install.packages("RSelenium")

# optional
install.packages("wdman") 

# Install all the drivers
wdman::selenium()

# Library
library(RSelenium)
```

```{r echo=FALSE, eval=TRUE}
# Library
library(RSelenium)
```

------------------------------------------------------------------------

### Collection (1)

```{r eval=FALSE}
# Creating the driver Firefox
remote_drive <- rsDriver(browser = "firefox",
                         chromever = NULL,
                         verbose = FALSE, # optional
                         port = 1123L # optional
                         ) 

# To do before navigating
remDr <- remote_drive$client

# Navigating
remDr$navigate(url_dyn)
```

------------------------------------------------------------------------

### Collection (2)

```{r eval=FALSE}
# Find the quote 
quotes <- remDr$findElement(using = "css selector",
                  "span.text")

# Get the content
quotes$getPageSource() %>% 
  read_html() %>% 
  html_text2()
```

## Advantages of RSelenium

Interactivity

```{r eval=FALSE}
# Write
remDr$sendKeysToActiveElement("a_key")

# Right click
remDr$click()

# Double click
remDr$doubleclick()

# Screenshot
remDr$screenshot()
```

## Limits of RSelenium

-   Dependency (Need Java)
    -   Not easily shareable
    -   Can be discouraging
-   Not beginner friendly
    -   Verbose
    -   Port management

## Port management example

If I want to open another web browser I need to indicate another port

```{r eval=FALSE}
remote_drive <- rsDriver(browser = "firefox",
                         chromever = NULL,
                         verbose = FALSE,
                         port = 4455L) # optional
```

# What is {hayalbaz}?

## {hayalbaz}

Author: [Colin Rundel](https://github.com/rundel)

[hayalbaz](https://github.com/rundel/hayalbaz)

"Puppeteer in a different language - this R package provides a puppeteer inspired interface to the Chrome Devtools Protocol using chromote."

-   What is Puppeteer?
-   What is chromote?

## Puppeteer

[Puppeteer](https://pptr.dev/) "Puppeteer is a Node.js library which provides a high-level API to control Chrome/Chromium over the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). Puppeteer runs in [headless](https://developer.chrome.com/articles/new-headless/) mode by default, but can be configured to run in full (non-headless) Chrome/Chromium."

## {chromote}

[chromote](https://github.com/rstudio/chromote) Chromote is an R implementation of the [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). It works with Chrome, Chromium, Opera, Vivaldi, and other browsers based on [Chromium](https://www.chromium.org/). By default it uses Google Chrome (which must already be installed on the system). To use a different browser, see [Specifying which browser to use](https://github.com/rstudio/chromote#specifying-which-browser-to-use).

Chromote is not the only R package that implements the Chrome DevTools Protocol. Here are some others: - [crrri](https://github.com/RLesur/crrri) by Romain Lesur and Christophe Dervieux - [decapitated](https://github.com/hrbrmstr/decapitated/) by Bob Rudis - [chradle](https://github.com/milesmcbain/chradle) by Miles McBain

The interface to Chromote is similar to [chrome-remote-interface](https://github.com/cyrus-and/chrome-remote-interface) for node.js.

# hayalbaz make scraping JavaScript heavy content easier

## Advantages of hayalbaz over RSelenium

1.  Easy installation
2.  Less verbose
3.  Easier overall
4.  Faster
5.  Good synergy with Rvest
6.  Promising feature

**Note: I don't think the goal of hayalbaz is to replace RSelenium, but it can be a valuable alternative**

# Advantages of hayalbaz over RSelenium

## 1. Easy installation

RSelenium: [The Complete RSelenium Installation Guide (2023)](https://www.youtube.com/watch?v=GnpJujF9dBw) - Need Java installed

hayalbaz:

```{r eval=FALSE}
# chromote
install.packages("chromote")

# hayalbaz
remotes::install_github("rundel/hayalbaz")
```

## 2. Less verbose (1)

```{r eval=FALSE}
# library
library(hayalbaz)

# Create the driver
driver <- puppet$new(url_dyn)

# quotes
driver$get_elements("span.text") %>% 
  html_text2() %>% 
  head()
```

## 2. Less verbose (2)

::: columns
::: {.column width="50%"}
rvest

```{r eval=FALSE}
# Collect html page
page <- read_html(url_dyn)

# Print quotes
page %>% 
  html_elements("span.text") %>% 
  html_text2() %>% 
  head()
```
:::

::: {.column width="50%"}
hayalbaz

```{r eval=FALSE}
# Collect html page
driver <- puppet$new(url_dyn)

# Print quotes
driver$get_elements("span.text") %>% 
  html_text2() %>% 
  head()
```
:::
:::

## 3. Faster (0)

Measuring the time used

```{r}
library(tictoc)
```

## 3. Faster (1)

Function for RSelenium

```{r eval=FALSE}
# Initialization
RSelenium_start <- function(){
  remote_driver <- rsDriver(browser = "firefox", 
                            chromever = NULL,
                            verbose = FALSE)
  
  remDr <- remote_driver$client
  
  return(remDr)
}

# Navigating
RSelenium_function <- function(url, remDr){
  remDr$navigate(url)
  
  result <- remDr$getPageSource()
  
  return(result)
}
```

## 3. Faster (2)

Function for hayalbaz

```{r eval=FALSE}
# Initialization
hayalbaz_start <- function(){
  driver <- puppet$new()
  
  return(driver)
}

# Navigating without waiting for the content
hayalbaz_function <- function(url, driver){
  driver$goto(url)
  
  result <- driver$get_source()
  
  return(result)
}

# Navigating waiting for the content
hayalbaz_function_2 <- function(url, driver){
  driver$goto(url)
  driver$wait_on_load()
  result <- driver$get_source()
  
  return(result)
}
```

## 3. Faster (3)

Time for initialization

::: columns
::: {.column width="50%"}
RSelenium

```{r eval=FALSE}
tic()
d1 <- RSelenium_start()
toc()
```

7.2 sec elapsed
:::

::: {.column width="50%"}
hayalbaz

```{r eval=FALSE}
tic()
d2 <- hayalbaz_start()
toc()
```

1.89 sec elapsed
:::
:::

## 3. Faster (4)

5 urls to scrap

```{r eval=FALSE}
urls <- c("https://en.wikipedia.org", 
          "https://www.google.com/", 
          "https://www.ricardo.ch",
          "https://www.imdb.com/",
          "https://www.accuweather.com/en")
```

## 3. Faster (5)

Scraping 5 web pages

::: columns
::: {.column width="50%"}
RSelenium

```{r eval=FALSE}
tic()
r1 <- map(urls, RSelenium_function, d1)
toc()
```

7.76 sec elapsed
:::

::: {.column width="50%"}
hayalbaz (no waiting)

```{r eval=FALSE}
tic()
r2 <- map(urls, hayalbaz_function, d2)
toc()
```

1.35 sec elapsed
:::
:::

## 3. Faster (6)

::: columns
::: {.column width="50%"}
RSelenium

```{r eval=FALSE}
tic()
r1 <- map(urls, RSelenium_function, d1)
toc()
```

7.76 sec elapsed
:::

::: {.column width="50%"}
hayalbaz (waiting)

```{r eval=FALSE}
tic()
r2 <- map(urls, hayalbaz_function_2, d2)
toc()
```

3.06 sec elapsed
:::
:::

RSelenium's object size = 688 bytes

hayalbaz' object size = 344 bytes

## 4. Easier overall

Because:

1.  Less installation
2.  Less verbose
3.  No port management
4.  One web browser: chrome (you can use chromium too though)
5.  Self-explanatory methods

## hayalbaz methods

```{r eval=FALSE}
# View the web browser
driver$view()

# Right click
driver$click()

# Screenshot
driver$screenshot()
```

## 5. Good synergy with rvest

hayalbaz

-   No need to use `read_html()` before using rvest

```{r eval=FALSE}
driver$get_elements("span.text") %>% 
  html_text2() %>% 
  head()
```

## 6. Promising feature

**Do you know the {shinytest2}?** To test applications automatically.

[shinytest2](https://rstudio.github.io/shinytest2/) uses [chromote](https://github.com/rstudio/chromote) to render applications in a headless Chrome browser. [chromote](https://github.com/rstudio/chromote) allows for a live preview, better debugging tools, and/or simply using modern JavaScript/CSS.

By simply recording your actions as code and extending them to test the more particular aspects of your application, it will result in fewer bugs and more confidence in future Shiny application development.

------------------------------------------------------------------------

The function

```{r eval=FALSE}
shinytest2::record_test()
```

------------------------------------------------------------------------

### Like puppeteer or playwright

Recording action to:

1.  Save time
2.  Be more beginner friendly

**It is still in the idea stage, but coming soon!**

**Again, the goal is not to replace RSelenium but it can be a valuable alternative**

# Thank you for your attention
